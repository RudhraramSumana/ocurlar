
[2]
20s
!pip install tensorflow==2.12.0
!pip install keras==2.12.0
!pip install tensorflow-addons==0.20.0
Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)
Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.70.0)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.12.1)
Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)
Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)
Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.6)
Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.1.0)
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)
Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)
Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.5.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.12.2)
Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)
Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)
Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)
Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)
Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)
Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)
Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)
Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)
Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)
Collecting tensorflow-addons==0.20.0
  Downloading tensorflow_addons-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.20.0) (24.2)
Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons==0.20.0)
  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)
Downloading tensorflow_addons-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 kB 9.4 MB/s eta 0:00:00
Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)
Installing collected packages: typeguard, tensorflow-addons
  Attempting uninstall: typeguard
    Found existing installation: typeguard 4.4.2
    Uninstalling typeguard-4.4.2:
      Successfully uninstalled typeguard-4.4.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.
Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3

[3]
0s
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(

[5]
24s
from google.colab import drive
drive.mount('/content/drive')
Mounted at /content/drive

[7]
25s
import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/ODIR-5K'):
    print(dirname)
/content/drive/MyDrive/ODIR-5K
/content/drive/MyDrive/ODIR-5K/full_df.csv (2)
/content/drive/MyDrive/ODIR-5K/train
/content/drive/MyDrive/ODIR-5K/test
/content/drive/MyDrive/ODIR-5K/preprocessed_images

[8]
0s
def has_cataract(text):
    if "cataract" in text:
        return 1
    else:
        return 0

[9]
0s
df = pd.read_csv('/content/drive/MyDrive/ODIR-5K/full_df.csv (2)/full_df.csv')

[10]
0s
def has_cataract(keywords):
    return 'cataract' in keywords.lower()

[11]
0s
df["left_cataract"] = df["Left-Diagnostic Keywords"].apply(lambda x: has_cataract(x))
df["right_cataract"] = df["Right-Diagnostic Keywords"].apply(lambda x: has_cataract(x))


[12]
0s
df["left_cataract"] = df["Left-Diagnostic Keywords"].apply(lambda x: has_cataract(x))
df["right_cataract"] = df["Right-Diagnostic Keywords"].apply(lambda x: has_cataract(x))

[13]
0s
left_cataract = df.loc[(df.C ==1) & (df.left_cataract == 1)]["Left-Fundus"].values
left_cataract[:15]
array(['0_left.jpg', '81_left.jpg', '103_left.jpg', '119_left.jpg',
       '254_left.jpg', '294_left.jpg', '330_left.jpg', '448_left.jpg',
       '465_left.jpg', '477_left.jpg', '553_left.jpg', '560_left.jpg',
       '594_left.jpg', '611_left.jpg', '625_left.jpg'], dtype=object)

[14]
0s
eft_cataract = df.loc[(df.C ==1) & (df.left_cataract == 1)]["Left-Fundus"].values
left_cataract[:15]
array(['0_left.jpg', '81_left.jpg', '103_left.jpg', '119_left.jpg',
       '254_left.jpg', '294_left.jpg', '330_left.jpg', '448_left.jpg',
       '465_left.jpg', '477_left.jpg', '553_left.jpg', '560_left.jpg',
       '594_left.jpg', '611_left.jpg', '625_left.jpg'], dtype=object)

[15]
0s
left_normal = df.loc[(df.C ==0) & (df["Left-Diagnostic Keywords"] == "normal fundus")]["Left-Fundus"].sample(250,random_state=42).values
right_normal = df.loc[(df.C ==0) & (df["Right-Diagnostic Keywords"] == "normal fundus")]["Right-Fundus"].sample(250,random_state=42).values
right_normal[:15]
array(['2964_right.jpg', '680_right.jpg', '500_right.jpg',
       '2368_right.jpg', '2820_right.jpg', '2769_right.jpg',
       '2696_right.jpg', '2890_right.jpg', '940_right.jpg',
       '2553_right.jpg', '3371_right.jpg', '3042_right.jpg',
       '919_right.jpg', '3427_right.jpg', '379_right.jpg'], dtype=object)

[16]
0s
left_cataract = df.loc[(df.C ==1) & (df.left_cataract == 1)]["Left-Fundus"].values
# Define the right_cataract variable similarly to left_cataract
right_cataract = df.loc[(df.C ==1) & (df.right_cataract == 1)]["Right-Fundus"].values
cataract = np.concatenate((left_cataract,right_cataract),axis=0)
normal = np.concatenate((left_normal,right_normal),axis=0)

[17]
0s
print(len(cataract),len(normal))
594 500

[18]
0s
import cv2
import random
from tqdm import tqdm
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import load_img,img_to_array
dataset_dir = "/content/drive/MyDrive/ODIR-5K/train"
image_size=224
labels = []
dataset = []
def create_dataset(image_category,label):
    for img in tqdm(image_category):
        image_path = os.path.join(dataset_dir,img)
        try:
            image = cv2.imread(image_path,cv2.IMREAD_COLOR)
            image = cv2.resize(image,(image_size,image_size))
        except:
            continue

        dataset.append([np.array(image),np.array(label)])
    random.shuffle(dataset)
    return dataset

[19]
2m
dataset = create_dataset(cataract,1)
len(dataset)
100%|██████████| 594/594 [04:10<00:00,  2.37it/s]
594

[20]
22s
dataset = create_dataset(normal,0)
len(dataset)
100%|██████████| 500/500 [00:22<00:00, 22.04it/s]
1094

[21]
4s
plt.figure(figsize=(12,7))
for i in range(10):
    sample = random.choice(range(len(dataset)))
    image = dataset[sample][0]
    category = dataset[sample][1]
    if category== 0:
        label = "Normal"
    else:
        label = "Cataract"
    plt.subplot(2,5,i+1)
    plt.imshow(image)
    plt.xlabel(label)
plt.tight_layout()

VGG16

[22]
0s
x = np.array([i[0] for i in dataset]).reshape(-1,image_size,image_size,3)
y = np.array([i[1] for i in dataset])

[23]
1s
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

[24]
3s
from tensorflow.keras.applications.vgg19 import VGG19
vgg = VGG19(weights="imagenet",include_top = False,input_shape=(image_size,image_size,3))
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5
80134624/80134624 [==============================] - 1s 0us/step

[25]
0s
for layer in vgg.layers:
    layer.trainable = False

[26]
0s
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten,Dense
model1 = Sequential()
model1.add(vgg)
model1.add(Flatten())
model1.add(Dense(1,activation="sigmoid"))
model1.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 vgg19 (Functional)          (None, 7, 7, 512)         20024384  
                                                                 
 flatten (Flatten)           (None, 25088)             0         
                                                                 
 dense (Dense)               (None, 1)                 25089     
                                                                 
=================================================================
Total params: 20,049,473
Trainable params: 25,089
Non-trainable params: 20,024,384
_________________________________________________________________

[27]
0s
model1.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])

[28]
0s
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping
# Removed the 'period' argument as it is deprecated and removed in newer TensorFlow versions.
# If you need to save every epoch, 'save_freq' with 'epoch' is the equivalent.
checkpoint = ModelCheckpoint("vgg19.h5",monitor="val_accuracy",verbose=1,save_best_only=True,
                             save_weights_only=False) # removed period=1
earlystop = EarlyStopping(monitor="val_accuracy",patience=5,verbose=1)
# Removed the 'period' argument as it is deprecated and removed in newer TensorFlow versions.
# If you need to save every epoch, 'save_freq' with 'epoch' is the equivalent.
checkpoint = ModelCheckpoint("vgg19.h5",monitor="val_accuracy",verbose=1,save_best_only=True,
                             save_weights_only=False) # removed period=1
earlystop = EarlyStopping(monitor="val_accuracy",patience=5,verbose=1)

[29]
22m
history1 = model1.fit(x_train,y_train,batch_size=32,epochs=10,validation_data=(x_test,y_test),
                    verbose=1,callbacks=[checkpoint,earlystop])
Epoch 1/10
28/28 [==============================] - ETA: 0s - loss: 1.6171 - accuracy: 0.8811 
Epoch 1: val_accuracy improved from -inf to 0.92694, saving model to vgg19.h5
28/28 [==============================] - 809s 29s/step - loss: 1.6171 - accuracy: 0.8811 - val_loss: 1.0367 - val_accuracy: 0.9269
Epoch 2/10
28/28 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.9577 
Epoch 2: val_accuracy improved from 0.92694 to 0.96347, saving model to vgg19.h5
28/28 [==============================] - 807s 29s/step - loss: 0.4431 - accuracy: 0.9577 - val_loss: 0.3615 - val_accuracy: 0.9635
Epoch 3/10
28/28 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9783 
Epoch 3: val_accuracy improved from 0.96347 to 0.96804, saving model to vgg19.h5
28/28 [==============================] - 799s 29s/step - loss: 0.0982 - accuracy: 0.9783 - val_loss: 0.3038 - val_accuracy: 0.9680
Epoch 4/10
28/28 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9931 
Epoch 4: val_accuracy did not improve from 0.96804
28/28 [==============================] - 741s 27s/step - loss: 0.0378 - accuracy: 0.9931 - val_loss: 0.2733 - val_accuracy: 0.9635
Epoch 5/10
28/28 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9931 
Epoch 5: val_accuracy improved from 0.96804 to 0.97717, saving model to vgg19.h5
28/28 [==============================] - 742s 27s/step - loss: 0.0259 - accuracy: 0.9931 - val_loss: 0.2616 - val_accuracy: 0.9772
Epoch 6/10
28/28 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.9646 
Epoch 6: val_accuracy did not improve from 0.97717
28/28 [==============================] - 792s 29s/step - loss: 0.3171 - accuracy: 0.9646 - val_loss: 0.4390 - val_accuracy: 0.9726
Epoch 7/10
28/28 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9897 
Epoch 7: val_accuracy improved from 0.97717 to 0.98174, saving model to vgg19.h5
28/28 [==============================] - 791s 28s/step - loss: 0.0701 - accuracy: 0.9897 - val_loss: 0.2411 - val_accuracy: 0.9817
Epoch 8/10
28/28 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9886 
Epoch 8: val_accuracy did not improve from 0.98174
28/28 [==============================] - 735s 26s/step - loss: 0.0520 - accuracy: 0.9886 - val_loss: 0.2927 - val_accuracy: 0.9817
Epoch 9/10
28/28 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9977 
Epoch 9: val_accuracy did not improve from 0.98174
28/28 [==============================] - 793s 29s/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.3212 - val_accuracy: 0.9817
Epoch 10/10
28/28 [==============================] - ETA: 0s - loss: 2.3111e-04 - accuracy: 1.0000 
Epoch 10: val_accuracy did not improve from 0.98174
28/28 [==============================] - 748s 27s/step - loss: 2.3111e-04 - accuracy: 1.0000 - val_loss: 0.3163 - val_accuracy: 0.9817

[30]
3m
loss,accuracy = model1.evaluate(x_test,y_test)
print("loss:",loss)
print("Accuracy:",accuracy)
7/7 [==============================] - 156s 21s/step - loss: 0.3163 - accuracy: 0.9817
loss: 0.31633350253105164
Accuracy: 0.9817351698875427

[31]
2m
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
y_pred1 = (model1.predict(x_test) > 0.5).astype("int32")
accuracy_score(y_test, y_pred1)
7/7 [==============================] - 164s 23s/step
0.9817351598173516

[32]
0s
print(classification_report(y_test,y_pred1))
              precision    recall  f1-score   support

           0       0.98      0.98      0.98       104
           1       0.98      0.98      0.98       115

    accuracy                           0.98       219
   macro avg       0.98      0.98      0.98       219
weighted avg       0.98      0.98      0.98       219


[33]
1s
from mlxtend.plotting import plot_confusion_matrix
cm = confusion_matrix(y_test,y_pred1)
plot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names = ["Normal","Cataract"],
                      show_normed = True);


[34]
1s
plt.style.use("ggplot")
fig = plt.figure(figsize=(12,6))
epochs = range(1,11)
plt.subplot(1,2,1)
plt.plot(epochs,history1.history["accuracy"],"go-")
plt.plot(epochs,history1.history["val_accuracy"],"ro-")
plt.title("Model Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Train","val"],loc = "upper left")

plt.subplot(1,2,2)
plt.plot(epochs,history1.history["loss"],"go-")
plt.plot(epochs,history1.history["val_loss"],"ro-")
plt.title("Model Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Train","val"],loc = "upper left")
plt.show()


[35]
5s
plt.figure(figsize=(12,7))
for i in range(10):
    sample = random.choice(range(len(x_test)))
    image = x_test[sample]
    category = y_test[sample]
    pred_category = y_pred1[sample]
    
    if category== 0:
        label = "Normal"
    else:
        label = "Cataract"
        
    if pred_category== 0:
        pred_label = "Normal"
    else:
        pred_label = "Cataract"
        
    plt.subplot(2,5,i+1)
    plt.imshow(image)
    plt.xlabel("Actual:{}\nPrediction:{}".format(label,pred_label))
plt.tight_layout() 

ResNet50

[36]
4s
from tensorflow.keras.applications.resnet50 import ResNet50
rnet = ResNet50(weights="imagenet",include_top = False,input_shape=(image_size,image_size,3))
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
94765736/94765736 [==============================] - 0s 0us/step

[37]
0s
for layer in rnet.layers:
    layer.trainable = False

[38]
1s
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten,Dense
model2 = Sequential()
model2.add(rnet)
model2.add(Flatten())
model2.add(Dense(1,activation="sigmoid"))
model2.summary()
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 resnet50 (Functional)       (None, 7, 7, 2048)        23587712  
                                                                 
 flatten_1 (Flatten)         (None, 100352)            0         
                                                                 
 dense_1 (Dense)             (None, 1)                 100353    
                                                                 
=================================================================
Total params: 23,688,065
Trainable params: 100,353
Non-trainable params: 23,587,712
_________________________________________________________________

[39]
0s
model2.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])

[40]
0s
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping
checkpoint = ModelCheckpoint("rnet50.h5",monitor="val_acc",verbose=1,save_best_only=True,
                             save_weights_only=False,period=1)
earlystop = EarlyStopping(monitor="val_acc",patience=5,verbose=1)
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.

[41]
21m
history2 = model2.fit(x_train,y_train,batch_size=32,epochs=10,validation_data=(x_test,y_test),
                    verbose=1,callbacks=[checkpoint,earlystop])
Epoch 1/10
28/28 [==============================] - ETA: 0s - loss: 1.3428 - accuracy: 0.8903WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 206s 7s/step - loss: 1.3428 - accuracy: 0.8903 - val_loss: 0.9712 - val_accuracy: 0.9132
Epoch 2/10
28/28 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9611WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 205s 7s/step - loss: 0.2516 - accuracy: 0.9611 - val_loss: 0.2959 - val_accuracy: 0.9726
Epoch 3/10
28/28 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9920WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 199s 7s/step - loss: 0.0385 - accuracy: 0.9920 - val_loss: 0.2019 - val_accuracy: 0.9726
Epoch 4/10
28/28 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9943WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 201s 7s/step - loss: 0.0333 - accuracy: 0.9943 - val_loss: 0.2271 - val_accuracy: 0.9589
Epoch 5/10
28/28 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9977WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 201s 7s/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.1469 - val_accuracy: 0.9817
Epoch 6/10
28/28 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9977WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 201s 7s/step - loss: 0.0046 - accuracy: 0.9977 - val_loss: 0.1796 - val_accuracy: 0.9680
Epoch 7/10
28/28 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9989WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 241s 9s/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.1642 - val_accuracy: 0.9772
Epoch 8/10
28/28 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9989WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 197s 7s/step - loss: 0.0019 - accuracy: 0.9989 - val_loss: 0.1719 - val_accuracy: 0.9863
Epoch 9/10
28/28 [==============================] - ETA: 0s - loss: 7.7094e-05 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 197s 7s/step - loss: 7.7094e-05 - accuracy: 1.0000 - val_loss: 0.1646 - val_accuracy: 0.9817
Epoch 10/10
28/28 [==============================] - ETA: 0s - loss: 3.3096e-05 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy
28/28 [==============================] - 243s 9s/step - loss: 3.3096e-05 - accuracy: 1.0000 - val_loss: 0.1646 - val_accuracy: 0.9817

[42]
53s
loss,accuracy = model2.evaluate(x_test,y_test)
print("loss:",loss)
print("Accuracy:",accuracy)
7/7 [==============================] - 53s 7s/step - loss: 0.1646 - accuracy: 0.9817
loss: 0.16458682715892792
Accuracy: 0.9817351698875427

[43]
1m
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
y_pred2 = (model2.predict(x_test) > 0.5).astype("int32")
accuracy_score(y_test, y_pred2)
7/7 [==============================] - 52s 6s/step
0.9817351598173516

[44]
0s
print(classification_report(y_test,y_pred2))
              precision    recall  f1-score   support

           0       0.98      0.98      0.98       104
           1       0.98      0.98      0.98       115

    accuracy                           0.98       219
   macro avg       0.98      0.98      0.98       219
weighted avg       0.98      0.98      0.98       219


[45]
1s
from mlxtend.plotting import plot_confusion_matrix
cm = confusion_matrix(y_test,y_pred2)
plot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names = ["Normal","Cataract"],
                      show_normed = True);


[46]
0s
plt.style.use("ggplot")
fig = plt.figure(figsize=(12,6))
epochs = range(1,11)
plt.subplot(1,2,1)
plt.plot(epochs,history2.history["accuracy"],"go-")
plt.plot(epochs,history2.history["val_accuracy"],"ro-")
plt.title("Model Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Train","val"],loc = "upper left")

plt.subplot(1,2,2)
plt.plot(epochs,history2.history["loss"],"go-")
plt.plot(epochs,history2.history["val_loss"],"ro-")
plt.title("Model Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Train","val"],loc = "upper left")
plt.show()


[47]
3s
plt.figure(figsize=(12,7))
for i in range(10):
    sample = random.choice(range(len(x_test)))
    image = x_test[sample]
    category = y_test[sample]
    pred_category = y_pred2[sample]
    
    if category== 0:
        label = "Normal"
    else:
        label = "Cataract"
        
    if pred_category== 0:
        pred_label = "Normal"
    else:
        pred_label = "Cataract"
        
    plt.subplot(2,5,i+1)
    plt.imshow(image)
    plt.xlabel("Actual:{}\nPrediction:{}".format(label,pred_label))
plt.tight_layout() 

Vision transformers

[48]
1s
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

[49]
3s
num_classes = 2
input_shape = (224, 224, 3)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

print(f"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")
x_train shape: (875, 224, 224, 3) - y_train shape: (875,)
x_test shape: (219, 224, 224, 3) - y_test shape: (219,)

[50]
1s
learning_rate = 0.001
weight_decay = 0.0001
batch_size = 32
image_size = 128 # We'll resize input images to this size
patch_size = 6  # Size of the patches to be extract from the input images
num_patches = (image_size // patch_size) ** 2
projection_dim = 64
num_heads = 4
transformer_units = [
    projection_dim * 2,
    projection_dim,
]  # Size of the transformer layers
transformer_layers = 6
mlp_head_units = [512, 256]  # Size of the dense layers of the final classifier

[51]
2s
data_augmentation = keras.Sequential(
    [
        layers.Normalization(),
        layers.Resizing(image_size, image_size),
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(factor=0.02),
        layers.RandomZoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name="data_augmentation",
)
# Compute the mean and the variance of the training data for normalization.
data_augmentation.layers[0].adapt(x_train)

[52]
0s
def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=tf.nn.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)
    return x

[53]
0s
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

[54]
10s
import matplotlib.pyplot as plt

plt.figure(figsize=(4, 4))
image = x_train[np.random.choice(range(x_train.shape[0]))]
plt.imshow(image.astype("uint8"))
plt.axis("off")

resized_image = tf.image.resize(
    tf.convert_to_tensor([image]), size=(image_size, image_size)
)
patches = Patches(patch_size)(resized_image)
print(f"Image size: {image_size} X {image_size}")
print(f"Patch size: {patch_size} X {patch_size}")
print(f"Patches per image: {patches.shape[1]}")
print(f"Elements per patch: {patches.shape[-1]}")

n = int(np.sqrt(patches.shape[1]))
plt.figure(figsize=(4, 4))
for i, patch in enumerate(patches[0]):
    ax = plt.subplot(n, n, i + 1)
    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))
    plt.imshow(patch_img.numpy().astype("uint8"))
    plt.axis("off")


[55]
0s
class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

[57]
0s
def create_vit_classifier():
    inputs = layers.Input(shape=input_shape)
    # Augment data.
    augmented = data_augmentation(inputs)
    # Create patches.
    patches = Patches(patch_size)(augmented)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.2)
    # Classify outputs.
    logits = layers.Dense(num_classes)(features)
    # Create the Keras model.
    model3 = keras.Model(inputs=inputs, outputs=logits)
    return model3

[58]
2s
model3 = create_vit_classifier()
model3.summary()
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               
                                )]                                                                
                                                                                                  
 data_augmentation (Sequential)  (None, 128, 128, 3)  7          ['input_3[0][0]']                
                                                                                                  
 patches_1 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      
                                                                                                  
 patch_encoder (PatchEncoder)   (None, 441, 64)      35200       ['patches_1[0][0]']              
                                                                                                  
 layer_normalization (LayerNorm  (None, 441, 64)     128         ['patch_encoder[0][0]']          
 alization)                                                                                       
                                                                                                  
 multi_head_attention (MultiHea  (None, 441, 64)     66368       ['layer_normalization[0][0]',    
 dAttention)                                                      'layer_normalization[0][0]']    
                                                                                                  
 add (Add)                      (None, 441, 64)      0           ['multi_head_attention[0][0]',   
                                                                  'patch_encoder[0][0]']          
                                                                                                  
 layer_normalization_1 (LayerNo  (None, 441, 64)     128         ['add[0][0]']                    
 rmalization)                                                                                     
                                                                                                  
 dense_3 (Dense)                (None, 441, 128)     8320        ['layer_normalization_1[0][0]']  
                                                                                                  
 dropout (Dropout)              (None, 441, 128)     0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 441, 64)      8256        ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 441, 64)      0           ['dense_4[0][0]']                
                                                                                                  
 add_1 (Add)                    (None, 441, 64)      0           ['dropout_1[0][0]',              
                                                                  'add[0][0]']                    
                                                                                                  
 layer_normalization_2 (LayerNo  (None, 441, 64)     128         ['add_1[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_1 (MultiH  (None, 441, 64)     66368       ['layer_normalization_2[0][0]',  
 eadAttention)                                                    'layer_normalization_2[0][0]']  
                                                                                                  
 add_2 (Add)                    (None, 441, 64)      0           ['multi_head_attention_1[0][0]', 
                                                                  'add_1[0][0]']                  
                                                                                                  
 layer_normalization_3 (LayerNo  (None, 441, 64)     128         ['add_2[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 dense_5 (Dense)                (None, 441, 128)     8320        ['layer_normalization_3[0][0]']  
                                                                                                  
 dropout_2 (Dropout)            (None, 441, 128)     0           ['dense_5[0][0]']                
                                                                                                  
 dense_6 (Dense)                (None, 441, 64)      8256        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 441, 64)      0           ['dense_6[0][0]']                
                                                                                                  
 add_3 (Add)                    (None, 441, 64)      0           ['dropout_3[0][0]',              
                                                                  'add_2[0][0]']                  
                                                                                                  
 layer_normalization_4 (LayerNo  (None, 441, 64)     128         ['add_3[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_2 (MultiH  (None, 441, 64)     66368       ['layer_normalization_4[0][0]',  
 eadAttention)                                                    'layer_normalization_4[0][0]']  
                                                                                                  
 add_4 (Add)                    (None, 441, 64)      0           ['multi_head_attention_2[0][0]', 
                                                                  'add_3[0][0]']                  
                                                                                                  
 layer_normalization_5 (LayerNo  (None, 441, 64)     128         ['add_4[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 dense_7 (Dense)                (None, 441, 128)     8320        ['layer_normalization_5[0][0]']  
                                                                                                  
 dropout_4 (Dropout)            (None, 441, 128)     0           ['dense_7[0][0]']                
                                                                                                  
 dense_8 (Dense)                (None, 441, 64)      8256        ['dropout_4[0][0]']              
                                                                                                  
 dropout_5 (Dropout)            (None, 441, 64)      0           ['dense_8[0][0]']                
                                                                                                  
 add_5 (Add)                    (None, 441, 64)      0           ['dropout_5[0][0]',              
                                                                  'add_4[0][0]']                  
                                                                                                  
 layer_normalization_6 (LayerNo  (None, 441, 64)     128         ['add_5[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_3 (MultiH  (None, 441, 64)     66368       ['layer_normalization_6[0][0]',  
 eadAttention)                                                    'layer_normalization_6[0][0]']  
                                                                                                  
 add_6 (Add)                    (None, 441, 64)      0           ['multi_head_attention_3[0][0]', 
                                                                  'add_5[0][0]']                  
                                                                                                  
 layer_normalization_7 (LayerNo  (None, 441, 64)     128         ['add_6[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 dense_9 (Dense)                (None, 441, 128)     8320        ['layer_normalization_7[0][0]']  
                                                                                                  
 dropout_6 (Dropout)            (None, 441, 128)     0           ['dense_9[0][0]']                
                                                                                                  
 dense_10 (Dense)               (None, 441, 64)      8256        ['dropout_6[0][0]']              
                                                                                                  
 dropout_7 (Dropout)            (None, 441, 64)      0           ['dense_10[0][0]']               
                                                                                                  
 add_7 (Add)                    (None, 441, 64)      0           ['dropout_7[0][0]',              
                                                                  'add_6[0][0]']                  
                                                                                                  
 layer_normalization_8 (LayerNo  (None, 441, 64)     128         ['add_7[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_4 (MultiH  (None, 441, 64)     66368       ['layer_normalization_8[0][0]',  
 eadAttention)                                                    'layer_normalization_8[0][0]']  
                                                                                                  
 add_8 (Add)                    (None, 441, 64)      0           ['multi_head_attention_4[0][0]', 
                                                                  'add_7[0][0]']                  
                                                                                                  
 layer_normalization_9 (LayerNo  (None, 441, 64)     128         ['add_8[0][0]']                  
 rmalization)                                                                                     
                                                                                                  
 dense_11 (Dense)               (None, 441, 128)     8320        ['layer_normalization_9[0][0]']  
                                                                                                  
 dropout_8 (Dropout)            (None, 441, 128)     0           ['dense_11[0][0]']               
                                                                                                  
 dense_12 (Dense)               (None, 441, 64)      8256        ['dropout_8[0][0]']              
                                                                                                  
 dropout_9 (Dropout)            (None, 441, 64)      0           ['dense_12[0][0]']               
                                                                                                  
 add_9 (Add)                    (None, 441, 64)      0           ['dropout_9[0][0]',              
                                                                  'add_8[0][0]']                  
                                                                                                  
 layer_normalization_10 (LayerN  (None, 441, 64)     128         ['add_9[0][0]']                  
 ormalization)                                                                                    
                                                                                                  
 multi_head_attention_5 (MultiH  (None, 441, 64)     66368       ['layer_normalization_10[0][0]', 
 eadAttention)                                                    'layer_normalization_10[0][0]'] 
                                                                                                  
 add_10 (Add)                   (None, 441, 64)      0           ['multi_head_attention_5[0][0]', 
                                                                  'add_9[0][0]']                  
                                                                                                  
 layer_normalization_11 (LayerN  (None, 441, 64)     128         ['add_10[0][0]']                 
 ormalization)                                                                                    
                                                                                                  
 dense_13 (Dense)               (None, 441, 128)     8320        ['layer_normalization_11[0][0]'] 
                                                                                                  
 dropout_10 (Dropout)           (None, 441, 128)     0           ['dense_13[0][0]']               
                                                                                                  
 dense_14 (Dense)               (None, 441, 64)      8256        ['dropout_10[0][0]']             
                                                                                                  
 dropout_11 (Dropout)           (None, 441, 64)      0           ['dense_14[0][0]']               
                                                                                                  
 add_11 (Add)                   (None, 441, 64)      0           ['dropout_11[0][0]',             
                                                                  'add_10[0][0]']                 
                                                                                                  
 layer_normalization_12 (LayerN  (None, 441, 64)     128         ['add_11[0][0]']                 
 ormalization)                                                                                    
                                                                                                  
 flatten_2 (Flatten)            (None, 28224)        0           ['layer_normalization_12[0][0]'] 
                                                                                                  
 dropout_12 (Dropout)           (None, 28224)        0           ['flatten_2[0][0]']              
                                                                                                  
 dense_15 (Dense)               (None, 512)          14451200    ['dropout_12[0][0]']             
                                                                                                  
 dropout_13 (Dropout)           (None, 512)          0           ['dense_15[0][0]']               
                                                                                                  
 dense_16 (Dense)               (None, 256)          131328      ['dropout_13[0][0]']             
                                                                                                  
 dropout_14 (Dropout)           (None, 256)          0           ['dense_16[0][0]']               
                                                                                                  
 dense_17 (Dense)               (None, 2)            514         ['dropout_14[0][0]']             
                                                                                                  
==================================================================================================
Total params: 15,117,577
Trainable params: 15,117,570
Non-trainable params: 7
__________________________________________________________________________________________________

[59]
0s
num_epochs = 30

[60]
1h
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping
optimizer = tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )

model3.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
        ],
    )

checkpoint_filepath = "/tmp/checkpoint"
checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,)
    
#earlystop = EarlyStopping(monitor="val_accuracy",patience=10,verbose=1)

history3 = model3.fit(
        x=x_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_split=0.1,
        callbacks=[checkpoint_callback],
    )

model3.load_weights(checkpoint_filepath)
_, accuracy = model3.evaluate(x_test, y_test)
print(f"Test accuracy: {round(accuracy * 100, 2)}%")
Epoch 1/30
25/25 [==============================] - 311s 11s/step - loss: 4.7529 - accuracy: 0.5070 - val_loss: 1.4286 - val_accuracy: 0.3977
Epoch 2/30
25/25 [==============================] - 284s 11s/step - loss: 1.8338 - accuracy: 0.5286 - val_loss: 1.5690 - val_accuracy: 0.5909
Epoch 3/30
25/25 [==============================] - 288s 12s/step - loss: 1.2924 - accuracy: 0.5820 - val_loss: 1.4042 - val_accuracy: 0.6705
Epoch 4/30
25/25 [==============================] - 279s 11s/step - loss: 1.0678 - accuracy: 0.5578 - val_loss: 0.5865 - val_accuracy: 0.7045
Epoch 5/30
25/25 [==============================] - 290s 12s/step - loss: 0.8035 - accuracy: 0.5947 - val_loss: 0.6091 - val_accuracy: 0.6591
Epoch 6/30
25/25 [==============================] - 279s 11s/step - loss: 0.7165 - accuracy: 0.6226 - val_loss: 0.5397 - val_accuracy: 0.7045
Epoch 7/30
25/25 [==============================] - 287s 11s/step - loss: 0.7718 - accuracy: 0.5909 - val_loss: 0.6934 - val_accuracy: 0.6591
Epoch 8/30
25/25 [==============================] - 281s 11s/step - loss: 0.7186 - accuracy: 0.6099 - val_loss: 0.5825 - val_accuracy: 0.7273
Epoch 9/30
25/25 [==============================] - 305s 12s/step - loss: 0.6523 - accuracy: 0.6353 - val_loss: 0.6258 - val_accuracy: 0.6364
Epoch 10/30
25/25 [==============================] - 293s 12s/step - loss: 0.6738 - accuracy: 0.6315 - val_loss: 0.6335 - val_accuracy: 0.6932
Epoch 11/30
25/25 [==============================] - 286s 11s/step - loss: 0.6197 - accuracy: 0.6696 - val_loss: 0.6510 - val_accuracy: 0.7386
Epoch 12/30
25/25 [==============================] - 289s 12s/step - loss: 0.5891 - accuracy: 0.6874 - val_loss: 0.5818 - val_accuracy: 0.6364
Epoch 13/30
25/25 [==============================] - 293s 12s/step - loss: 0.5723 - accuracy: 0.7039 - val_loss: 0.5337 - val_accuracy: 0.7273
Epoch 14/30
25/25 [==============================] - 284s 11s/step - loss: 0.5264 - accuracy: 0.7294 - val_loss: 0.5873 - val_accuracy: 0.6932
Epoch 15/30
25/25 [==============================] - 289s 12s/step - loss: 0.5344 - accuracy: 0.7294 - val_loss: 0.5178 - val_accuracy: 0.7386
Epoch 16/30
25/25 [==============================] - 289s 12s/step - loss: 0.5312 - accuracy: 0.7433 - val_loss: 0.5275 - val_accuracy: 0.7045
Epoch 17/30
25/25 [==============================] - 289s 12s/step - loss: 0.5249 - accuracy: 0.7421 - val_loss: 0.5455 - val_accuracy: 0.7045
Epoch 18/30
25/25 [==============================] - 296s 12s/step - loss: 0.5092 - accuracy: 0.7421 - val_loss: 0.6213 - val_accuracy: 0.6818
Epoch 19/30
25/25 [==============================] - 295s 12s/step - loss: 0.5166 - accuracy: 0.7446 - val_loss: 0.5153 - val_accuracy: 0.7386
Epoch 20/30
25/25 [==============================] - 296s 12s/step - loss: 0.4730 - accuracy: 0.7687 - val_loss: 0.6336 - val_accuracy: 0.6364
Epoch 21/30
25/25 [==============================] - 285s 11s/step - loss: 0.4602 - accuracy: 0.7967 - val_loss: 0.5880 - val_accuracy: 0.6477
Epoch 22/30
25/25 [==============================] - 284s 11s/step - loss: 0.5221 - accuracy: 0.7497 - val_loss: 0.4825 - val_accuracy: 0.7727
Epoch 23/30
25/25 [==============================] - 287s 11s/step - loss: 0.4708 - accuracy: 0.7675 - val_loss: 0.4744 - val_accuracy: 0.7500
Epoch 24/30
25/25 [==============================] - 278s 11s/step - loss: 0.4428 - accuracy: 0.8018 - val_loss: 0.5779 - val_accuracy: 0.7500
Epoch 25/30
25/25 [==============================] - 291s 12s/step - loss: 0.4241 - accuracy: 0.7903 - val_loss: 0.5710 - val_accuracy: 0.6705
Epoch 26/30
25/25 [==============================] - 283s 11s/step - loss: 0.4433 - accuracy: 0.7916 - val_loss: 0.4210 - val_accuracy: 0.7500
Epoch 27/30
25/25 [==============================] - 296s 12s/step - loss: 0.4880 - accuracy: 0.7789 - val_loss: 0.5524 - val_accuracy: 0.6932
Epoch 28/30
25/25 [==============================] - 288s 12s/step - loss: 0.4327 - accuracy: 0.8069 - val_loss: 0.5400 - val_accuracy: 0.6932
Epoch 29/30
25/25 [==============================] - 289s 12s/step - loss: 0.4202 - accuracy: 0.8119 - val_loss: 0.5112 - val_accuracy: 0.7500
Epoch 30/30
25/25 [==============================] - 284s 11s/step - loss: 0.3554 - accuracy: 0.8475 - val_loss: 0.5439 - val_accuracy: 0.7045
7/7 [==============================] - 25s 3s/step - loss: 0.5279 - accuracy: 0.7215
Test accuracy: 72.15%

[61]
1m
loss, accuracy = model3.evaluate(x_test,y_test)
print("loss:",loss)
print("Accuracy:",accuracy)
7/7 [==============================] - 44s 6s/step - loss: 0.5279 - accuracy: 0.7215
loss: 0.5278547406196594
Accuracy: 0.7214611768722534

[62]
0s
acc = history3.history['accuracy']
val_acc = history3.history['val_accuracy']

loss = history3.history['loss']
val_loss = history3.history['val_loss']

[63]
0s
len(val_acc),len(val_loss),len(acc),len(loss)
(30, 30, 30, 30)
Double-click (or enter) to edit


[64]
1s
import matplotlib.pyplot as plt
EPOCHS = 30

plt.style.use("ggplot")
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(range(EPOCHS), history3.history['accuracy'],"go-")
plt.plot(range(EPOCHS), history3.history['val_accuracy'],"ro-")
plt.title("Model Accuracy")
plt.xlabel("Epochs")


Comparison

[65]
0s
accloss = {'Model':['VGG19','ResNet50','Vision Transformer'],
           'Training_Accuracy':[max(history1.history['accuracy']),max(history2.history['accuracy']),max(history3.history['accuracy'])],
           'Training_Loss':[min(history1.history['loss']),min(history2.history['loss']),min(history3.history['loss'])],
           'Validation_Accuracy':[max(history1.history['val_accuracy']),max(history2.history['val_accuracy']),max(history3.history['val_accuracy'])],
           'Validation_Loss':[min(history1.history['val_loss']),min(history2.history['val_loss']),min(history3.history['val_loss'])]}

[66]
0s
accloss
{'Model': ['VGG19', 'ResNet50', 'Vision Transformer'],
 'Training_Accuracy': [1.0, 1.0, 0.8475222587585449],
 'Training_Loss': [0.0002311124699190259,
  3.309588646516204e-05,
  0.3554236590862274],
 'Validation_Accuracy': [0.9817351698875427,
  0.9863013625144958,
  0.7727272510528564],
 'Validation_Loss': [0.241132915019989,
  0.14691856503486633,
  0.4210472106933594]}

[67]
0s
comp = pd.DataFrame.from_dict(accloss)
comp

Next steps:
Colab paid products - Cancel contracts here
